{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d544e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed6845e",
   "metadata": {},
   "source": [
    "# Preprocessing des datas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0da838cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "def get_landmarks(image_rgb, pose_model): \n",
    "    '''\n",
    "    Récupère les landmarks d'une image donnée en RGB via un modèle déjà chargé.\n",
    "    Return: un numpy array (33,4) ou None si rien n'est trouvé.\n",
    "    '''\n",
    "\n",
    "    results = pose_model.process(image_rgb)\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "\n",
    "        pose_np = np.array([[lm.x, lm.y, lm.z, lm.visibility] for lm in results.pose_landmarks.landmark])\n",
    "        return pose_np\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "009d424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(folder_path, path_save, pickle_name=\"dataset.pkl\"): \n",
    "    ''' récupère les landmarks d'images dans un dossier via la fonction get_landmarks(image)\n",
    "\n",
    "    folder_path : dossier où sont les images\n",
    "    path_save   : dossier où sauvegarder le pickle\n",
    "    pickle_name : nom du fichier pickle\n",
    "\n",
    "    return: numpy array de shape (N, 132) si flatten (33*4)\n",
    "    '''\n",
    "\n",
    "    mp_pose = mp.solutions.pose\n",
    "    os.makedirs(path_save, exist_ok=True)\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        data_list = []\n",
    "        fichiers = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        fichiers.sort()\n",
    "\n",
    "        path_save_pickle = os.path.join(path_save, pickle_name)\n",
    "\n",
    "        for name in fichiers:\n",
    "            img_path = os.path.join(folder_path, name)\n",
    "            img = cv2.imread(img_path)\n",
    "\n",
    "            if img is None:\n",
    "                print(f\"Impossible de lire : {img_path}\")\n",
    "                continue\n",
    "\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            landmarks_vector = get_landmarks(img_rgb, pose)\n",
    "\n",
    "            if landmarks_vector is not None:\n",
    "                data_list.append(landmarks_vector.flatten())  # (132,)\n",
    "            else:\n",
    "                print(f\"Ignorée (pas de pose détectée) : {name}\")\n",
    "\n",
    "        dataset = np.array(data_list, dtype=np.float32)\n",
    "\n",
    "        with open(path_save_pickle, 'wb') as f:\n",
    "            pickle.dump(dataset, f)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d69193",
   "metadata": {},
   "source": [
    "## attributs target  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27d4623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(dataset, class_int): \n",
    "    ''''récupère la target du dataset'''\n",
    "\n",
    "    nb_images = dataset.shape[0]\n",
    "    class_id = class_int\n",
    "    target = np.full(nb_images, class_id)\n",
    "\n",
    "    return target \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec2f1c1",
   "metadata": {},
   "source": [
    "# Build dataset et concatenisation en numpy array#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0c950bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pickle\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def build_full_dataset(data_root, path_save, poses=(\"cobra\", \"tree\", \"downdog\",\"forwardfold\",\"chair\",\n",
    "                                                   \"warrior2\",\"warrior3\",\"plank\",\"lotus\")):\n",
    "    \"\"\"\n",
    "    Construit X, y à partir de dossiers:\n",
    "      data_root/cobra, data_root/tree, data_root/downdog\n",
    "\n",
    "    Retour:\n",
    "      X (N,132), y (N,)\n",
    "    + sauvegarde un pickle par pose (comme vous faites déjà)\n",
    "    \"\"\"\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    # mapping figé par l'ordre de `poses` pour être sur que les numéros des classes correspndent bien aux poses\n",
    "    label2id = {pose_name: i for i, pose_name in enumerate(poses)}\n",
    "    id2label = {i: pose_name for pose_name, i in label2id.items()}\n",
    "\n",
    "    for pose_name in poses:\n",
    "        class_id = label2id[pose_name]\n",
    "        folder_path = os.path.join(data_root, pose_name)\n",
    "\n",
    "\n",
    "        ds = get_dataset(\n",
    "            folder_path=folder_path,\n",
    "            path_save=path_save,\n",
    "            pickle_name=f\"dataset_{pose_name}.pkl\"\n",
    "        )\n",
    "\n",
    "        target = get_target(ds, class_id)\n",
    "\n",
    "        X_list.append(ds)\n",
    "        y_list.append(target)\n",
    "\n",
    "    X = np.vstack(X_list).astype(np.float32)\n",
    "    y = np.concatenate(y_list).astype(np.int64)\n",
    "    \n",
    "    os.makedirs(path_save, exist_ok=True)\n",
    "\n",
    "    # (optionnel) sauvegarde globale\n",
    "    with open(os.path.join(path_save, \"X_all.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(X, f)\n",
    "    with open(os.path.join(path_save, \"y_all.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(y, f)\n",
    "    \n",
    "    ## sauvegarde du mapping (facilité pour les appels pour la suite)\n",
    "    mapping = {\n",
    "        \"labels\": [id2label[i] for i in sorted(id2label)],  # ordre = sorties du modèle\n",
    "        \"label2id\": label2id,\n",
    "        \"id2label\": {str(k): v for k, v in id2label.items()}\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(path_save, \"labels_mapping.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(mapping, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # sanity check : compte par classe réellement présente\n",
    "    counts = Counter(y.tolist())\n",
    "    print(\"\\n Counts par id:\", dict(counts))\n",
    "    print(\"id 0 =\", id2label.get(0), \"| id 1 =\", id2label.get(1),\"| id 2 =\", id2label.get(2),\n",
    "          \"| id 3 =\", id2label.get(3), \"| id 4 =\", id2label.get(4),\n",
    "          \"| id 5 =\", id2label.get(5), \"| id 6 =\", id2label.get(6), \"| id 7 =\", id2label.get(7),\n",
    "          \"|  id 8 =\", id2label.get(8))\n",
    "        \n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ad46e",
   "metadata": {},
   "source": [
    "# Dataset des postures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1570f255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignorée (pas de pose détectée) : 1_152.jpg\n",
      "Ignorée (pas de pose détectée) : images69.jpg\n",
      "Ignorée (pas de pose détectée) : 197.jpg\n",
      "Ignorée (pas de pose détectée) : File49.jpg\n",
      "Ignorée (pas de pose détectée) : PXL_20251217_103643021.MP.jpg\n",
      "Ignorée (pas de pose détectée) : images145.jpg\n",
      "Ignorée (pas de pose détectée) : downdog (20).jpg\n",
      "Ignorée (pas de pose détectée) : downdog (20).png\n",
      "Ignorée (pas de pose détectée) : downdog (24).jpg\n",
      "Ignorée (pas de pose détectée) : downdog (78).jpg\n",
      "Ignorée (pas de pose détectée) : File66.jpeg\n",
      "Ignorée (pas de pose détectée) : File12.png\n",
      "Ignorée (pas de pose détectée) : File17.png\n",
      "Ignorée (pas de pose détectée) : File2.png\n",
      "Ignorée (pas de pose détectée) : File27.png\n",
      "Ignorée (pas de pose détectée) : File31.png\n",
      "Ignorée (pas de pose détectée) : File4.png\n",
      "Ignorée (pas de pose détectée) : File5.png\n",
      "Ignorée (pas de pose détectée) : File62.png\n",
      "Ignorée (pas de pose détectée) : File67.png\n",
      "Ignorée (pas de pose détectée) : File79.jpeg\n",
      "Ignorée (pas de pose détectée) : File4.png\n",
      "Ignorée (pas de pose détectée) : File60.jpeg\n",
      "Ignorée (pas de pose détectée) : File22.png\n",
      "Ignorée (pas de pose détectée) : File34.png\n",
      "Ignorée (pas de pose détectée) : File41.png\n",
      "Ignorée (pas de pose détectée) : File28.png\n",
      "Ignorée (pas de pose détectée) : File41.png\n",
      "Ignorée (pas de pose détectée) : File46.png\n",
      "Ignorée (pas de pose détectée) : File5.png\n",
      "Ignorée (pas de pose détectée) : File54.jpg\n",
      "Ignorée (pas de pose détectée) : File65.jpeg\n",
      "Ignorée (pas de pose détectée) : File3.png\n",
      "\n",
      " Counts par id: {0: 77, 1: 67, 2: 91, 3: 55, 4: 49, 5: 54, 6: 51, 7: 43, 8: 55}\n",
      "id 0 = cobra | id 1 = tree | id 2 = downdog | id 3 = forwardfold | id 4 = chair | id 5 = warrior2 | id 6 = warrior3 | id 7 = plank |  id 8 = lotus\n",
      "(542, 132) (542,)\n"
     ]
    }
   ],
   "source": [
    "path_data = r'C:\\Users\\mvana\\Documents\\Formation data scientist\\20. ACV\\Posture_yoga\\data'\n",
    "path_save = \"datasets\"\n",
    "\n",
    "X, y = build_full_dataset(path_data, path_save)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcefad1",
   "metadata": {},
   "source": [
    "# Vérification de la sortie #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa630bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(542,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chemin_du_fichier = r'C:\\Users\\mvana\\Documents\\Formation data scientist\\20. ACV\\ACV_yoga\\datasets\\Y_all.pkl'\n",
    "\n",
    "with open(chemin_du_fichier, 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0ca71ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(542,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_target(dataset, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d26f611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset ### classe des images traitées dans y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d8d37",
   "metadata": {},
   "source": [
    "## création des logos en numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "794ecac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chair.png', 'child.png', 'cobra.png', 'downdog.png', 'forwardfold.png', 'lotus.jpg', 'plank.jpg', 'tree.jpg', 'warrior1.png', 'warrior2.jpg', 'warrior3.jpg']\n"
     ]
    }
   ],
   "source": [
    "labels = [\"cobra\",\"tree\",\"downdog\",\"forwardfold\",\"chair\",\"warrior2\",\"warrior3\",\"plank\",\"child\",\"lotus\"]\n",
    "logos_dir = r\"C:\\Users\\mvana\\Documents\\Formation data scientist\\20. ACV\\Posture_yoga\\data\\logo\"\n",
    "\n",
    "import os\n",
    "print(os.listdir(logos_dir))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69311022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def load_logo_as_numpy(img_path, size=(128,128)):\n",
    "    img_bgr = cv2.imread(img_path)\n",
    "    if img_bgr is None:\n",
    "        return None\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    if size is not None:\n",
    "        img_rgb = cv2.resize(img_rgb, size)\n",
    "    return img_rgb.astype(np.uint8)\n",
    "\n",
    "def find_logo_file(logos_dir, label):\n",
    "    exts = (\".png\", \".jpg\", \".jpeg\", \".webp\")\n",
    "    label_low = label.lower()\n",
    "\n",
    "    candidates = []\n",
    "    for f in os.listdir(logos_dir):\n",
    "        f_low = f.lower()\n",
    "        if f_low.endswith(exts):\n",
    "            base = os.path.splitext(f_low)[0]  # nom sans extension\n",
    "            # match exact OU \"contient\"\n",
    "            if base == label_low or label_low in base:\n",
    "                candidates.append(f)\n",
    "\n",
    "    if len(candidates) == 0:\n",
    "        return None\n",
    "\n",
    "    # prend le 1er par ordre alphabétique (stable)\n",
    "    candidates.sort()\n",
    "    return os.path.join(logos_dir, candidates[0])\n",
    "\n",
    "def save_logos_npz(logos_dir, labels, out_npz_path, size=(128,128)):\n",
    "    arrays = {}\n",
    "    chosen_files = {}\n",
    "\n",
    "    for label in labels:\n",
    "        img_path = find_logo_file(logos_dir, label)\n",
    "        if img_path is None:\n",
    "            print(f\"[WARN] Logo manquant pour '{label}'\")\n",
    "            continue\n",
    "\n",
    "        arr = load_logo_as_numpy(img_path, size=size)\n",
    "        if arr is None:\n",
    "            print(f\"[WARN] Impossible de lire: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        arrays[label] = arr\n",
    "        chosen_files[label] = os.path.basename(img_path)\n",
    "\n",
    "    np.savez_compressed(out_npz_path, **arrays)\n",
    "    return list(arrays.keys()), chosen_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f86a801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ logos.npz créé: artifacts\\logos.npz\n",
      "✅ logos trouvés: ['cobra', 'tree', 'downdog', 'forwardfold', 'chair', 'warrior2', 'warrior3', 'plank', 'child', 'lotus']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "labels = [\"cobra\",\"tree\",\"downdog\",\"forwardfold\",\"chair\",\"warrior2\",\"warrior3\",\"plank\",\"child\",\"lotus\"]\n",
    "logos_dir = r\"C:\\Users\\mvana\\Documents\\Formation data scientist\\20. ACV\\Posture_yoga\\data\\logo\"\n",
    "\n",
    "out_dir = \"artifacts\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "npz_path = os.path.join(out_dir, \"logos.npz\")\n",
    "\n",
    "kept, chosen_files = save_logos_npz(logos_dir, labels, npz_path, size=(128,128))\n",
    "\n",
    "with open(\"yoga_config.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "cfg[\"logos_npz_path\"] = npz_path.replace(\"\\\\\", \"/\")\n",
    "cfg[\"logos_npz_keys\"] = kept\n",
    "cfg[\"logos_files_used\"] = chosen_files  # pratique pour debug\n",
    "\n",
    "with open(\"yoga_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cfg, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ logos.npz créé:\", npz_path)\n",
    "print(\"✅ logos trouvés:\", kept)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
