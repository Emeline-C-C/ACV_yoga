{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc49a2d",
   "metadata": {},
   "source": [
    "## Modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8cac5b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b02464ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_pickle('datasets/X_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6f84ae85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(542, 132)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0ae81fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels ordre modèle: ['cobra', 'tree', 'downdog', 'forwardfold', 'chair', 'warrior2', 'warrior3', 'plank', 'lotus']\n",
      "id2label: {'0': 'cobra', '1': 'tree', '2': 'downdog', '3': 'forwardfold', '4': 'chair', '5': 'warrior2', '6': 'warrior3', '7': 'plank', '8': 'lotus'}\n"
     ]
    }
   ],
   "source": [
    "import json, pickle\n",
    "\n",
    "with open(\"datasets/labels_mapping.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mp = json.load(f)\n",
    "\n",
    "print(\"labels ordre modèle:\", mp[\"labels\"])\n",
    "print(\"id2label:\", mp[\"id2label\"])  # clés en str\n",
    "\n",
    "\n",
    "with open(\"datasets/y_all.pkl\", \"rb\") as f:\n",
    "    y = pickle.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "61d2f6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'cobra',\n",
       " 1: 'tree',\n",
       " 2: 'downdog',\n",
       " 3: 'forwardfold',\n",
       " 4: 'chair',\n",
       " 5: 'warrior2',\n",
       " 6: 'warrior3',\n",
       " 7: 'plank',\n",
       " 8: 'lotus'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {int(k): v for k, v in mp[\"id2label\"].items()}\n",
    "label2id = mp.get(\"label2id\", None)  # celui-ci a des clés string (= labels) donc OK\n",
    "\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "caecc90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(542,)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9cad6849",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape((y.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "164a3205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(542, 1)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f39cef",
   "metadata": {},
   "source": [
    "## Model split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "713ea6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d9d8a337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363, 132)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4afcd129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 132)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7648b717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((179, 1), (363, 1))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape, y_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e631f1",
   "metadata": {},
   "source": [
    "## Model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "77316bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(254, activation=\"relu\", input_shape=(132,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(10, activation=\"softmax\") ## 10 classes\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ed483460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.1460 - loss: 2.2468 - val_accuracy: 0.2235 - val_loss: 2.0706\n",
      "Epoch 2/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.2452 - loss: 2.0654 - val_accuracy: 0.4749 - val_loss: 1.8390\n",
      "Epoch 3/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.3388 - loss: 1.8398 - val_accuracy: 0.5251 - val_loss: 1.5716\n",
      "Epoch 4/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4160 - loss: 1.6275 - val_accuracy: 0.6536 - val_loss: 1.3738\n",
      "Epoch 5/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5014 - loss: 1.4213 - val_accuracy: 0.5978 - val_loss: 1.1400\n",
      "Epoch 6/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5565 - loss: 1.2760 - val_accuracy: 0.7598 - val_loss: 0.9709\n",
      "Epoch 7/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6033 - loss: 1.1161 - val_accuracy: 0.7877 - val_loss: 0.8396\n",
      "Epoch 8/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6501 - loss: 0.9952 - val_accuracy: 0.7765 - val_loss: 0.7410\n",
      "Epoch 9/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6529 - loss: 0.9461 - val_accuracy: 0.7654 - val_loss: 0.7474\n",
      "Epoch 10/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7052 - loss: 0.8237 - val_accuracy: 0.8547 - val_loss: 0.5772\n",
      "Epoch 11/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7741 - loss: 0.7053 - val_accuracy: 0.8324 - val_loss: 0.5254\n",
      "Epoch 12/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7603 - loss: 0.6625 - val_accuracy: 0.8045 - val_loss: 0.5177\n",
      "Epoch 13/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7658 - loss: 0.6592 - val_accuracy: 0.8324 - val_loss: 0.4767\n",
      "Epoch 14/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7989 - loss: 0.5898 - val_accuracy: 0.8492 - val_loss: 0.4353\n",
      "Epoch 15/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8375 - loss: 0.5030 - val_accuracy: 0.8771 - val_loss: 0.3807\n",
      "Epoch 16/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8127 - loss: 0.4746 - val_accuracy: 0.8827 - val_loss: 0.3596\n",
      "Epoch 17/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8485 - loss: 0.4116 - val_accuracy: 0.9218 - val_loss: 0.2967\n",
      "Epoch 18/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8209 - loss: 0.4718 - val_accuracy: 0.8771 - val_loss: 0.3395\n",
      "Epoch 19/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8375 - loss: 0.4257 - val_accuracy: 0.8994 - val_loss: 0.3075\n",
      "Epoch 20/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8623 - loss: 0.3689 - val_accuracy: 0.9330 - val_loss: 0.3116\n",
      "Epoch 21/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8760 - loss: 0.3771 - val_accuracy: 0.8994 - val_loss: 0.3089\n",
      "Epoch 22/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8953 - loss: 0.2850 - val_accuracy: 0.9385 - val_loss: 0.2425\n",
      "Epoch 23/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8871 - loss: 0.3360 - val_accuracy: 0.9050 - val_loss: 0.2863\n",
      "Epoch 24/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8843 - loss: 0.2873 - val_accuracy: 0.9385 - val_loss: 0.2466\n",
      "Epoch 25/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9008 - loss: 0.3008 - val_accuracy: 0.9330 - val_loss: 0.2736\n",
      "Epoch 26/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8926 - loss: 0.2880 - val_accuracy: 0.9330 - val_loss: 0.2412\n",
      "Epoch 27/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9146 - loss: 0.2389 - val_accuracy: 0.9162 - val_loss: 0.2596\n",
      "Epoch 28/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9229 - loss: 0.2400 - val_accuracy: 0.9274 - val_loss: 0.2440\n",
      "Epoch 29/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9146 - loss: 0.2628 - val_accuracy: 0.9553 - val_loss: 0.2049\n",
      "Epoch 30/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9366 - loss: 0.2022 - val_accuracy: 0.9497 - val_loss: 0.2099\n",
      "Epoch 31/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9229 - loss: 0.2239 - val_accuracy: 0.9274 - val_loss: 0.2168\n",
      "Epoch 32/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9256 - loss: 0.2071 - val_accuracy: 0.9274 - val_loss: 0.2229\n",
      "Epoch 33/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9201 - loss: 0.2029 - val_accuracy: 0.9274 - val_loss: 0.2086\n",
      "Epoch 34/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9394 - loss: 0.1919 - val_accuracy: 0.9162 - val_loss: 0.2290\n",
      "Epoch 35/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9201 - loss: 0.1977 - val_accuracy: 0.9553 - val_loss: 0.1947\n",
      "Epoch 36/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9477 - loss: 0.1817 - val_accuracy: 0.9274 - val_loss: 0.2394\n",
      "Epoch 37/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9366 - loss: 0.1789 - val_accuracy: 0.9274 - val_loss: 0.2405\n",
      "Epoch 38/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9284 - loss: 0.2026 - val_accuracy: 0.9274 - val_loss: 0.2144\n",
      "Epoch 39/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9311 - loss: 0.1904 - val_accuracy: 0.9441 - val_loss: 0.1925\n",
      "Epoch 40/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9394 - loss: 0.1599 - val_accuracy: 0.9385 - val_loss: 0.2163\n",
      "Epoch 41/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9697 - loss: 0.1151 - val_accuracy: 0.9553 - val_loss: 0.1657\n",
      "Epoch 42/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9532 - loss: 0.1399 - val_accuracy: 0.9441 - val_loss: 0.1747\n",
      "Epoch 43/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9532 - loss: 0.1365 - val_accuracy: 0.9330 - val_loss: 0.1877\n",
      "Epoch 44/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9284 - loss: 0.1757 - val_accuracy: 0.9497 - val_loss: 0.1729\n",
      "Epoch 45/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9477 - loss: 0.1513 - val_accuracy: 0.9385 - val_loss: 0.2056\n",
      "Epoch 46/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9614 - loss: 0.1189 - val_accuracy: 0.9385 - val_loss: 0.2048\n",
      "Epoch 47/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9449 - loss: 0.1405 - val_accuracy: 0.9553 - val_loss: 0.2011\n",
      "Epoch 48/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9532 - loss: 0.1320 - val_accuracy: 0.9553 - val_loss: 0.1642\n",
      "Epoch 49/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9532 - loss: 0.1231 - val_accuracy: 0.9553 - val_loss: 0.1722\n",
      "Epoch 50/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9614 - loss: 0.1387 - val_accuracy: 0.9218 - val_loss: 0.2298\n",
      "Epoch 51/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9669 - loss: 0.1152 - val_accuracy: 0.9609 - val_loss: 0.1420\n",
      "Epoch 52/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9587 - loss: 0.1134 - val_accuracy: 0.9665 - val_loss: 0.1708\n",
      "Epoch 53/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9614 - loss: 0.1204 - val_accuracy: 0.9274 - val_loss: 0.2684\n",
      "Epoch 54/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9449 - loss: 0.1204 - val_accuracy: 0.9609 - val_loss: 0.1860\n",
      "Epoch 55/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9697 - loss: 0.1060 - val_accuracy: 0.9385 - val_loss: 0.1989\n",
      "Epoch 56/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9669 - loss: 0.0921 - val_accuracy: 0.9609 - val_loss: 0.1561\n",
      "Epoch 57/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9752 - loss: 0.0793 - val_accuracy: 0.9497 - val_loss: 0.1577\n",
      "Epoch 58/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9835 - loss: 0.0757 - val_accuracy: 0.9609 - val_loss: 0.1330\n",
      "Epoch 59/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9752 - loss: 0.0974 - val_accuracy: 0.9441 - val_loss: 0.1606\n",
      "Epoch 60/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9587 - loss: 0.1147 - val_accuracy: 0.9050 - val_loss: 0.2615\n",
      "Epoch 61/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9559 - loss: 0.1244 - val_accuracy: 0.9609 - val_loss: 0.1473\n",
      "Epoch 62/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9725 - loss: 0.0815 - val_accuracy: 0.9609 - val_loss: 0.1339\n",
      "Epoch 63/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9669 - loss: 0.0880 - val_accuracy: 0.9609 - val_loss: 0.1755\n",
      "Epoch 64/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9669 - loss: 0.0809 - val_accuracy: 0.9609 - val_loss: 0.1580\n",
      "Epoch 65/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9752 - loss: 0.0915 - val_accuracy: 0.9609 - val_loss: 0.1703\n",
      "Epoch 66/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9752 - loss: 0.0775 - val_accuracy: 0.9330 - val_loss: 0.2031\n",
      "Epoch 67/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9725 - loss: 0.0879 - val_accuracy: 0.9218 - val_loss: 0.2099\n",
      "Epoch 68/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9669 - loss: 0.0923 - val_accuracy: 0.9553 - val_loss: 0.1755\n",
      "Epoch 69/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9642 - loss: 0.1238 - val_accuracy: 0.9609 - val_loss: 0.1568\n",
      "Epoch 70/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9725 - loss: 0.0783 - val_accuracy: 0.9553 - val_loss: 0.1676\n",
      "Epoch 71/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9807 - loss: 0.0728 - val_accuracy: 0.9665 - val_loss: 0.1649\n",
      "Epoch 72/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9697 - loss: 0.0916 - val_accuracy: 0.9553 - val_loss: 0.1811\n",
      "Epoch 73/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9614 - loss: 0.1047 - val_accuracy: 0.9609 - val_loss: 0.1670\n",
      "Epoch 74/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9780 - loss: 0.0824 - val_accuracy: 0.9553 - val_loss: 0.1455\n",
      "Epoch 75/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9807 - loss: 0.0640 - val_accuracy: 0.9665 - val_loss: 0.1734\n",
      "Epoch 76/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9752 - loss: 0.0742 - val_accuracy: 0.9609 - val_loss: 0.1506\n",
      "Epoch 77/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9917 - loss: 0.0516 - val_accuracy: 0.9441 - val_loss: 0.1798\n",
      "Epoch 78/500\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9835 - loss: 0.0438 - val_accuracy: 0.9553 - val_loss: 0.1613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23834d82f00>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",     # ce qu'on surveille\n",
    "    patience=20,             # nb d'époques sans amélioration\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "50645d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "pose_id = np.argmax(pred, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "dd88ab21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 5, 1, 3, 1, 2, 8, 8, 8, 0, 2, 6, 0, 8, 3, 7, 1, 8, 0, 2, 5,\n",
       "       1, 5, 8, 0, 1, 6, 0, 2, 2, 2, 6, 3, 8, 8, 7, 0, 4, 5, 7, 3, 2, 2,\n",
       "       2, 1, 2, 2, 4, 0, 5, 8, 8, 0, 8, 2, 1, 6, 3, 4, 3, 4, 0, 8, 0, 8,\n",
       "       7, 5, 4, 5, 1, 2, 3, 2, 0, 4, 0, 2, 6, 1, 3, 0, 1, 5, 0, 8, 8, 0,\n",
       "       7, 1, 1, 7, 6, 6, 1, 1, 2, 0, 3, 2, 8, 2, 0, 8, 2, 0, 1, 8, 8, 4,\n",
       "       2, 8, 5, 4, 5, 6, 4, 1, 3, 8, 2, 8, 2, 5, 0, 5, 4, 5, 6, 7, 8, 1,\n",
       "       5, 3, 2, 7, 5, 7, 7, 2, 6, 6, 0, 5, 7, 4, 7, 5, 7, 7, 0, 7, 0, 4,\n",
       "       4, 2, 4, 2, 1, 8, 8, 0, 1, 6, 3, 6, 0, 2, 0, 0, 8, 0, 2, 3, 3, 2,\n",
       "       1, 2, 6])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0b933061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.06702664e-07, 3.04429355e-04, 2.82738472e-10, ...,\n",
       "        2.14697931e-08, 8.64445315e-09, 2.97582875e-10],\n",
       "       [9.97182965e-01, 1.62399061e-07, 4.26969609e-06, ...,\n",
       "        2.74508144e-03, 3.11473450e-05, 6.67125306e-08],\n",
       "       [2.38733513e-08, 6.47679772e-05, 1.14178812e-10, ...,\n",
       "        1.32989051e-08, 8.24766810e-10, 9.47328674e-11],\n",
       "       ...,\n",
       "       [1.60280445e-06, 9.95685458e-01, 5.99650150e-07, ...,\n",
       "        1.06050902e-06, 3.19197607e-05, 2.11367102e-08],\n",
       "       [4.44552645e-10, 4.02756062e-13, 9.99933243e-01, ...,\n",
       "        5.28175406e-05, 3.30198833e-15, 9.32308918e-12],\n",
       "       [4.95626637e-07, 3.78587650e-08, 1.17133322e-06, ...,\n",
       "        1.65728095e-04, 8.03929284e-11, 2.30547510e-08]], dtype=float32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b7f9e0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 132)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0:1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f259cf5c",
   "metadata": {},
   "source": [
    "## Model Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8485432b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.1330\n",
      "Test accuracy (keras): 0.9609\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Évaluation Keras\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy (keras): {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "99aebf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix:\n",
      " [[28  0  0  0  0  0  0  1  2]\n",
      " [ 0 20  0  0  0  0  0  0  0]\n",
      " [ 0  0 30  0  0  0  0  0  0]\n",
      " [ 0  0  0 13  0  0  0  0  0]\n",
      " [ 0  0  0  0 14  0  1  1  0]\n",
      " [ 0  0  0  0  0 17  0  0  0]\n",
      " [ 0  0  0  0  0  0 13  0  0]\n",
      " [ 0  0  0  1  0  1  0 13  0]\n",
      " [ 0  0  0  0  0  0  0  0 24]]\n"
     ]
    }
   ],
   "source": [
    "# Accuracy \"sklearn\" + matrice de confusion (plus explicite)\n",
    "y_proba = model.predict(X_test, verbose=0)\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bea8a3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       cobra       1.00      0.90      0.95        31\n",
      "        tree       1.00      1.00      1.00        20\n",
      "     downdog       1.00      1.00      1.00        30\n",
      " forwardfold       0.93      1.00      0.96        13\n",
      "       chair       1.00      0.88      0.93        16\n",
      "    warrior2       0.94      1.00      0.97        17\n",
      "    warrior3       0.93      1.00      0.96        13\n",
      "       plank       0.87      0.87      0.87        15\n",
      "       lotus       0.92      1.00      0.96        24\n",
      "\n",
      "    accuracy                           0.96       179\n",
      "   macro avg       0.95      0.96      0.96       179\n",
      "weighted avg       0.96      0.96      0.96       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, target_names=[id2label[i] for i in sorted(id2label)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0184e8b",
   "metadata": {},
   "source": [
    "## Prediction du modèle avec une photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "607ff231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import mediapipe as mp\n",
    "from mediapipe import solutions\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b75fc406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_root = r'C:\\Users\\mvana\\Documents\\Formation data scientist\\20. ACV\\Posture_yoga\\data'\n",
    "img_path = os.path.join(data_root, 'downdog', 'downdog (24).jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fb037456",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_landmarks(image_rgb, pose_model): \n",
    "    '''\n",
    "    Récupère les landmarks d'une image donnée en RGB via un modèle déjà chargé.\n",
    "    Return: un numpy array (33,4) ou None si rien n'est trouvé.\n",
    "    '''\n",
    "\n",
    "    results = pose_model.process(image_rgb)\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "\n",
    "        pose_np = np.array([[lm.x, lm.y, lm.z, lm.visibility] for lm in results.pose_landmarks.landmark])\n",
    "        return pose_np\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e7e2dfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image chargée: True\n"
     ]
    }
   ],
   "source": [
    "arbre = cv2.imread(img_path)\n",
    "\n",
    "print(\"Image chargée:\", arbre is not None)\n",
    "image_rgb = cv2.cvtColor(arbre, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b656108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose_model:\n",
    "\n",
    "    landmarks = get_landmarks(image_rgb,pose_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1f160624",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_flatten = landmarks.flatten().reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ba263172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n"
     ]
    }
   ],
   "source": [
    "results = model.predict(landmarks_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6c831738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(3)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db5780d",
   "metadata": {},
   "source": [
    "Optin facilité pour récupérer facilement une image et la prédire vite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ca2d867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "def predict_pose_from_path(img_path, model, id2label, min_det=0.5, min_track=0.5):\n",
    "    \"\"\"\n",
    "    Prédit la pose à partir d'un chemin d'image.\n",
    "    Retour: (label, proba, pred_id, probs)\n",
    "    \"\"\"\n",
    "    img_bgr = cv2.imread(img_path)\n",
    "    if img_bgr is None:\n",
    "        raise FileNotFoundError(f\"Impossible de lire l'image: {img_path}\")\n",
    "\n",
    "    image_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=min_det, min_tracking_confidence=min_track) as pose_model:\n",
    "        landmarks = get_landmarks(image_rgb, pose_model)\n",
    "\n",
    "    if landmarks is None:\n",
    "        return \"no_pose_detected\", 0.0, None, None\n",
    "\n",
    "    X = landmarks.flatten().reshape(1, -1).astype(np.float32)  # (1,132)\n",
    "\n",
    "    probs = model.predict(X, verbose=0)[0]   # (num_classes,)\n",
    "    pred_id = int(np.argmax(probs))\n",
    "    label = id2label[pred_id]\n",
    "    proba = float(probs[pred_id])\n",
    "\n",
    "    return label, proba, pred_id, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d6a6618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def predict_on_folder_random(folder_path, model, id2label, n=10, seed=None):\n",
    "    files = [f for f in os.listdir(folder_path)\n",
    "             if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "    if len(files) == 0:\n",
    "        print(\"Aucune image trouvée.\")\n",
    "        return\n",
    "\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    # Tire n fichiers distincts au hasard (ou tous si n > nb images)\n",
    "    chosen = random.sample(files, k=min(n, len(files)))\n",
    "\n",
    "    for f in chosen:\n",
    "        p = os.path.join(folder_path, f)\n",
    "        label, proba, pred_id, _ = predict_pose_from_path(p, model, id2label)\n",
    "        print(f\"{f:30s} -> {label:12s}  ({proba:.3f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8d91a3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️ forwardfold (proba=0.983, id=3)\n"
     ]
    }
   ],
   "source": [
    "data_root = r'C:\\Users\\mvana\\Documents\\Formation data scientist\\20. ACV\\Posture_yoga\\data'\n",
    "img_path = os.path.join(data_root, 'downdog', 'downdog (19).jpg')\n",
    "\n",
    "label, proba, pred_id, probs = predict_pose_from_path(img_path, model, id2label)\n",
    "print(\"➡️\", label, f\"(proba={proba:.3f}, id={pred_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "914a5013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️ forwardfold (proba=0.982, id=3)\n"
     ]
    }
   ],
   "source": [
    "data_root = r'C:\\Users\\mvana\\Documents\\Formation data scientist\\20. ACV\\Posture_yoga\\data'\n",
    "img_path = os.path.join(data_root, 'child', 'File27.png')\n",
    "\n",
    "label, proba, pred_id, probs = predict_pose_from_path(img_path, model, id2label)\n",
    "print(\"➡️\", label, f\"(proba={proba:.3f}, id={pred_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "727dcfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COBRA ===\n",
      "3_322.jpg                      -> cobra         (0.551)\n",
      "1_152.jpg                      -> cobra         (0.981)\n",
      "File33.png                     -> cobra         (0.993)\n",
      "images35.jpg                   -> cobra         (0.994)\n",
      "PXL_20251217_103534696.MP.jpg  -> cobra         (0.926)\n",
      "\n",
      "=== TREE ===\n",
      "97.jpg                         -> tree          (0.999)\n",
      "images133.jpg                  -> warrior2      (0.562)\n",
      "00000124.jpg                   -> tree          (0.996)\n",
      "00000007 (1).jpg               -> tree          (0.995)\n",
      "152.jpg                        -> tree          (0.993)\n",
      "\n",
      "=== DOWNDOG ===\n",
      "downdog (80).jpg               -> forwardfold   (0.830)\n",
      "downdog (51).jpg               -> downdog       (1.000)\n",
      "downdog (20).jpg               -> forwardfold   (0.836)\n",
      "downdog (6).jpg                -> downdog       (1.000)\n",
      "downdog (21).png               -> downdog       (0.885)\n",
      "\n",
      "=== FORWARDFOLD ===\n",
      "File1.png                      -> forwardfold   (0.996)\n",
      "File55.png                     -> no_pose_detected  (0.000)\n",
      "File47.png                     -> forwardfold   (1.000)\n",
      "File14.png                     -> forwardfold   (0.995)\n",
      "File13.png                     -> forwardfold   (1.000)\n",
      "\n",
      "=== CHAIR ===\n",
      "File61.png                     -> chair         (0.771)\n",
      "File39.png                     -> chair         (0.998)\n",
      "File13.png                     -> chair         (1.000)\n",
      "File20.png                     -> chair         (1.000)\n",
      "File63.png                     -> chair         (0.999)\n",
      "\n",
      "=== WARRIOR2 ===\n",
      "File50.png                     -> warrior2      (1.000)\n",
      "File43.png                     -> warrior2      (0.867)\n",
      "File5.png                      -> warrior2      (0.999)\n",
      "File14.png                     -> warrior2      (1.000)\n",
      "File57.jpeg                    -> warrior2      (0.997)\n",
      "\n",
      "=== WARRIOR3 ===\n",
      "File42.png                     -> warrior3      (1.000)\n",
      "File57.png                     -> warrior3      (1.000)\n",
      "File12.png                     -> warrior3      (0.999)\n",
      "File6.png                      -> warrior3      (1.000)\n",
      "File68.jpeg                    -> warrior3      (0.997)\n",
      "\n",
      "=== PLANK ===\n",
      "File62.jpeg                    -> plank         (0.898)\n",
      "File8.png                      -> plank         (0.996)\n",
      "File16.png                     -> plank         (0.896)\n",
      "File15.png                     -> plank         (0.951)\n",
      "File58.jpeg                    -> plank         (0.941)\n",
      "\n",
      "=== LOTUS ===\n",
      "File62.png                     -> no_pose_detected  (0.000)\n",
      "File22.png                     -> lotus         (1.000)\n",
      "File5.png                      -> lotus         (0.998)\n",
      "File42.png                     -> lotus         (1.000)\n",
      "File57.png                     -> lotus         (1.000)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def test_all_classes(data_root, model, id2label, poses, n_per_class=5, seed=None):\n",
    "    \"\"\"\n",
    "    Pour chaque pose (dossier), tire n_per_class images au hasard et affiche la prédiction.\n",
    "    \"\"\"\n",
    "    for pose in poses:\n",
    "        folder = os.path.join(data_root, pose)\n",
    "        print(f\"\\n=== {pose.upper()} ===\")\n",
    "\n",
    "        if not os.path.isdir(folder):\n",
    "            print(f\"[WARN] Dossier introuvable: {folder}\")\n",
    "            continue\n",
    "\n",
    "        predict_on_folder_random(folder, model, id2label, n=n_per_class, seed=seed)\n",
    "\n",
    "# Exemple d'utilisation\n",
    "poses = [\"cobra\", \"tree\", \"downdog\", \"forwardfold\", \"chair\",\n",
    "         \"warrior2\", \"warrior3\", \"plank\", \"lotus\"]\n",
    "\n",
    "test_all_classes(data_root, model, id2label, poses, n_per_class=5, seed=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ac52a",
   "metadata": {},
   "source": [
    "## Save the model in pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1ecee3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('yoga_model_9pose.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e44de45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Config écrite: yoga_config.json\n",
      "labels: ['cobra', 'tree', 'downdog', 'forwardfold', 'chair', 'warrior2', 'warrior3', 'plank', 'lotus']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Crée la liste labels dans l'ordre des ids (TRÈS important)\n",
    "labels = [id2label[i] for i in sorted(id2label.keys())]\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"model_path\": \"yoga_model_9pose.keras\", \n",
    "    \"num_classes\": len(labels),\n",
    "    \"labels\": labels,                       # ordre = indices de sortie du modèle\n",
    "    \"id2label\": {str(k): v for k, v in id2label.items()},   # JSON => clés string\n",
    "}\n",
    "\n",
    "config_path = os.path.join(\"yoga_config.json\")\n",
    "\n",
    "with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ Config écrite:\", config_path)\n",
    "print(\"labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2225126b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_path': 'yoga_model_10pose.keras',\n",
       " 'num_classes': 9,\n",
       " 'labels': ['cobra',\n",
       "  'tree',\n",
       "  'downdog',\n",
       "  'forwardfold',\n",
       "  'chair',\n",
       "  'warrior2',\n",
       "  'warrior3',\n",
       "  'plank',\n",
       "  'lotus'],\n",
       " 'id2label': {'0': 'cobra',\n",
       "  '1': 'tree',\n",
       "  '2': 'downdog',\n",
       "  '3': 'forwardfold',\n",
       "  '4': 'chair',\n",
       "  '5': 'warrior2',\n",
       "  '6': 'warrior3',\n",
       "  '7': 'plank',\n",
       "  '8': 'lotus'}}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeeb857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
